{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import dependency_treebank\n",
    "from nltk.grammar import DependencyGrammar\n",
    "from nltk.parse import DependencyGraph\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Habitat:\n",
    "    oid = -1\n",
    "    name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./BioNLP-OST-2019_BB-norm_train/\"\n",
    "test_path = \"./BioNLP-OST-2019_BB-norm_test/\"\n",
    "dev_path = \"./BioNLP-OST-2019_BB-norm_dev/\"\n",
    "BB_train = os.listdir(train_path)\n",
    "BB_test = os.listdir(test_path)\n",
    "BB_dev = os.listdir(dev_path)\n",
    "obp_file = \"OntoBiotope_BioNLP-OST-2019.obo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology = open(obp_file,encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BB_train_a1 = sorted([name for name in BB_train if name.endswith(\"a1\")])\n",
    "BB_train_a2 = sorted([name for name in BB_train if name.endswith(\"a2\")])\n",
    "BB_train_txt = sorted([name for name in BB_train if name.endswith(\"txt\")])\n",
    "\n",
    "BB_dev_a1 = sorted([name for name in BB_dev if name.endswith(\"a1\")])\n",
    "BB_dev_a2 = sorted([name for name in BB_dev if name.endswith(\"a2\")])\n",
    "BB_dev_txt = sorted([name for name in BB_dev if name.endswith(\"txt\")])\n",
    "\n",
    "BB_test_a1 = sorted([name for name in BB_test if name.endswith(\"a1\")])\n",
    "BB_test_txt = sorted([name for name in BB_test if name.endswith(\"txt\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_phrase(phrase):\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_phrase(phrase):\n",
    "    #print(type(phrase))\n",
    "    #print(phrase)\n",
    "    \n",
    "    phrase = phrase.split(\" \")\n",
    "    for i in range(len(phrase)):\n",
    "        phrase[i] = ps.stem(phrase[i])\n",
    "    phrase = \" \".join(phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_phrase(phrase):\n",
    "    #print(type(phrase))\n",
    "    #print(phrase)\n",
    "    \n",
    "    phrase = phrase.split(\" \")\n",
    "    for i in range(len(phrase)):\n",
    "        phrase[i] = wnl.lemmatize(phrase[i])\n",
    "    phrase = \" \".join(phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_reducer = lemmatize_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_habitat_map(processor):\n",
    "    habitat_list = ontology.split(\"\\n\\n[Term]\\n\")[1:]\n",
    "    habitat_map = {}\n",
    "    habitat_map_originals = {}\n",
    "    for h in habitat_list:\n",
    "        #new_habitat = Habitat()\n",
    "        h = h.split(\"\\n\")\n",
    "        oid = re.findall(r\"(?<=id: OBT:)[0-9]+\", h[0])[0]\n",
    "        name = (re.findall(r\"(?<=name: ).+\", h[1])[0])\n",
    "        if name != name.upper():\n",
    "            name = name.lower()\n",
    "            name = processor(name)\n",
    "    \n",
    "        habitat_map[name] = oid\n",
    "        habitat_map_originals[oid] = name\n",
    "    \n",
    "        for h_line in h:\n",
    "            if \"synonym\" in h_line:\n",
    "                synonym_name = (re.findall(r\"(?<=synonym: \\\").+(?=\\\")\", h_line)[0])\n",
    "                if synonym_name != synonym_name.upper():\n",
    "                    synonym_name = synonym_name.lower()\n",
    "                    synonym_name = processor(synonym_name)\n",
    "                if \"EXACT\" in h_line and synonym_name not in habitat_map:\n",
    "                    habitat_map[synonym_name] = oid\n",
    "    return habitat_map, habitat_map_originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "habitat_map, habitat_map_originals = generate_habitat_map(phrase_reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_habitats(a1, a2, processor):\n",
    "    a2 = a2.split(\"\\n\")\n",
    "    a1 = a1.split(\"\\n\")\n",
    "    \n",
    "    for annotation in a2:\n",
    "        a1_line = 0\n",
    "        t = \"\"\n",
    "        oid = \"\"\n",
    "        #print(annotation)\n",
    "        if \"OntoBiotope\" in annotation:\n",
    "            t = re.findall(r\"(?<=Annotation:)T[0-9]+\", annotation)[0]\n",
    "            oid = re.findall(r\"(?<=Referent:OBT:)[0-9]+\", annotation)[0]\n",
    "            named_entity = \"\"\n",
    "            \n",
    "            while t not in a1[a1_line] and a1_line < len(a1)-1:\n",
    "                a1_line += 1\n",
    "            if \"Habitat\" in a1[a1_line]: \n",
    "                named_entity = (a1[a1_line].split(\"\\t\")[2])\n",
    "                if named_entity != named_entity.upper():\n",
    "                    named_entity = named_entity.lower()\n",
    "                    named_entity = processor(named_entity)\n",
    "                if named_entity not in habitat_map:\n",
    "                    habitat_map[named_entity] = oid\n",
    "                    print(\"Newly added entity:\", named_entity, oid)\n",
    "                    print(\"Original entity:\", habitat_map_originals[oid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(BB_train_txt)):    \n",
    "    a1 = open(train_path + BB_train_a1[i],encoding=\"utf8\").read()\n",
    "    a2 = open(train_path + BB_train_a2[i],encoding=\"utf8\").read()\n",
    "    add_habitats(a1, a2, phrase_reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_habitats(a1, txt, a1_name, processor):\n",
    "    cands = 0\n",
    "    matches = 0\n",
    "    match_list = {}\n",
    "    a1 = a1.split(\"\\n\")\n",
    "    for line in a1:\n",
    "        if \"Habitat\" in line:\n",
    "            cands += 1\n",
    "            name = (line.split(\"\\t\")[2])\n",
    "            named_entity = name\n",
    "            if name != name.upper():\n",
    "                named_entity = name.lower()\n",
    "                named_entity = processor(named_entity)\n",
    "            if named_entity in habitat_map:\n",
    "                matches += 1\n",
    "                match = name + \" - \" + habitat_map[named_entity]\n",
    "                match_list[line.split(\"\\t\")[0]] = [habitat_map[named_entity], named_entity]\n",
    "    print(a1_name)\n",
    "    print(\"Out of\", cands, \"candidates,\", matches, \"matches found\")\n",
    "    print()\n",
    "    return match_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_test = {}\n",
    "for i in range(len(BB_dev_txt)):\n",
    "    a1 = open(dev_path + BB_dev_a1[i],encoding=\"utf8\").read()\n",
    "    txt = open(dev_path + BB_dev_txt[i],encoding=\"utf8\").read() \n",
    "    annotations_test[BB_dev_a1[i]] = predict_habitats(a1, txt, BB_dev_a1[i], phrase_reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_habitats(a1, a2, processor):\n",
    "    a2 = a2.split(\"\\n\")\n",
    "    a1 = a1.split(\"\\n\")\n",
    "    match_list = {}\n",
    "    for annotation in a2:\n",
    "        a1_line = 0\n",
    "        t = \"\"\n",
    "        oid = \"\"\n",
    "        #print(annotation)\n",
    "        if \"OntoBiotope\" in annotation:\n",
    "            t = re.findall(r\"(?<=Annotation:)T[0-9]+\", annotation)[0]\n",
    "            oid = re.findall(r\"(?<=Referent:OBT:)[0-9]+\", annotation)[0]\n",
    "            named_entity = \"\"\n",
    "            while t not in a1[a1_line] and a1_line < len(a1)-1:\n",
    "                a1_line += 1\n",
    "            if \"Habitat\" in a1[a1_line]:\n",
    "                named_entity = (a1[a1_line].split(\"\\t\")[2])\n",
    "                if named_entity != named_entity.upper():\n",
    "                    named_entity = named_entity.lower()\n",
    "                    named_entity = processor(named_entity)\n",
    "                match_list[t] = [oid, named_entity]\n",
    "    return match_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_a2 = {}\n",
    "for i in range(len(BB_dev_txt)):\n",
    "    a1 = open(dev_path + BB_dev_a1[i],encoding=\"utf8\").read()\n",
    "    a2 = open(dev_path + BB_dev_a2[i],encoding=\"utf8\").read()\n",
    "    dev_a2[BB_dev_a1[i]] = test_habitats(a1,a2, phrase_reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_pred_count = 0\n",
    "false_pred_count = 0\n",
    "total_count = 0\n",
    "for key in dev_a2:\n",
    "    \"\"\"print()\n",
    "    print(key)\n",
    "    print(dev_a2[key])\n",
    "    print(\"**************\")\"\"\"\n",
    "    total_count += len(dev_a2[key])\n",
    "    if key in annotations_test:\n",
    "        for pred in annotations_test[key]:            \n",
    "            if annotations_test[key][pred][0] == dev_a2[key][pred][0]:\n",
    "                true_pred_count += 1\n",
    "            else:\n",
    "                print(key)\n",
    "                print(\"Prediction:\", annotations_test[key][pred],\" Real:\", dev_a2[key][pred])\n",
    "                false_pred_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = true_pred_count/total_count\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = true_pred_count/(true_pred_count + false_pred_count)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(treebank.parsed_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_prod = set([production for parsed_sent in treebank.parsed_sents() for production in parsed_sent.productions()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = dependency_treebank.parsed_sents()[1].to_conll(3)\n",
    "print(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = DependencyGraph(bb)\n",
    "dg.tree().pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import CoreNLPParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = CoreNLPParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(parser.parse('What is the airspeed of an unladen swallow ?'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.corenlp import CoreNLPDependencyParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_parser = CoreNLPDependencyParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parses = dep_parser.parse(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaaa = [[(governor, dep, dependent) for governor, dep, dependent in parse.triples()] for parse in parses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaaa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"birds supplemented with the n-hexane extract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
