{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import dependency_treebank\n",
    "from nltk.grammar import DependencyGrammar\n",
    "from nltk.parse import DependencyGraph\n",
    "import pprint\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "dep_parser = CoreNLPDependencyParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Habitat:\n",
    "    oid = -1\n",
    "    name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./BioNLP-OST-2019_BB-norm_train/\"\n",
    "test_path = \"./BioNLP-OST-2019_BB-norm_test/\"\n",
    "dev_path = \"./BioNLP-OST-2019_BB-norm_dev/\"\n",
    "BB_train = os.listdir(train_path)\n",
    "BB_test = os.listdir(test_path)\n",
    "BB_dev = os.listdir(dev_path)\n",
    "obp_file = \"OntoBiotope_BioNLP-OST-2019.obo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology = open(obp_file,encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BB_train_a1 = sorted([name for name in BB_train if name.endswith(\"a1\")])\n",
    "BB_train_a2 = sorted([name for name in BB_train if name.endswith(\"a2\")])\n",
    "BB_train_txt = sorted([name for name in BB_train if name.endswith(\"txt\")])\n",
    "\n",
    "BB_dev_a1 = sorted([name for name in BB_dev if name.endswith(\"a1\")])\n",
    "BB_dev_a2 = sorted([name for name in BB_dev if name.endswith(\"a2\")])\n",
    "BB_dev_txt = sorted([name for name in BB_dev if name.endswith(\"txt\")])\n",
    "\n",
    "BB_test_a1 = sorted([name for name in BB_test if name.endswith(\"a1\")])\n",
    "BB_test_txt = sorted([name for name in BB_test if name.endswith(\"txt\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_phrase(phrase):\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_phrase(phrase):\n",
    "    #print(type(phrase))\n",
    "    #print(phrase)\n",
    "    \n",
    "    phrase = phrase.split(\" \")\n",
    "    for i in range(len(phrase)):\n",
    "        phrase[i] = ps.stem(phrase[i])\n",
    "    phrase = \" \".join(phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_phrase(phrase):\n",
    "    #print(type(phrase))\n",
    "    #print(phrase)\n",
    "    \n",
    "    phrase = phrase.split(\" \")\n",
    "    for i in range(len(phrase)):\n",
    "        phrase[i] = wnl.lemmatize(phrase[i])\n",
    "    phrase = \" \".join(phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headword(phrase):\n",
    "    #print(phrase)\n",
    "    if phrase == phrase.upper():\n",
    "        return \"\"\n",
    "    \n",
    "    parses = dep_parser.parse(phrase.split())\n",
    "    triples = [[(governor, dep, dependent) for governor, dep, dependent in parse.triples()] for parse in parses]\n",
    "    \n",
    "    try:\n",
    "        if(len(triples) > 0 and len(triples[0]) > 0 and len(triples[0][0]) > 0 and len(triples[0][0][0]) > 0 and triples[0][0][0][0] is not None):\n",
    "            return triples[0][0][0][0]\n",
    "        else:\n",
    "            return phrase\n",
    "    except:\n",
    "        return phrase\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_documents(txt, processor):\n",
    "    document = []\n",
    "    txt = processor(txt)\n",
    "    sentence_list = sent_tokenize(txt)\n",
    "    \n",
    "    for sentence in sentence_list:\n",
    "        word_list = word_tokenize(sentence)\n",
    "        word_list = [processor(word) for word in word_list if word not in stopwords.words(\"english\") and word.isalnum()]\n",
    "        for i in range(len(word_list)):\n",
    "            if word_list[i].isnumeric():\n",
    "                word_list[i] = \"##number##\"\n",
    "        document.append(word_list)\n",
    "    #print(document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i in range(len(BB_train_txt)):\n",
    "    txt = open(train_path + BB_train_txt[i],encoding=\"utf8\").read()\n",
    "    documents += add_documents(txt, phrase_reducer)\n",
    "for i in range(len(BB_dev_txt)):\n",
    "    txt = open(dev_path + BB_dev_txt[i],encoding=\"utf8\").read()\n",
    "    documents += add_documents(txt, phrase_reducer)\n",
    "for i in range(len(BB_test_txt)):\n",
    "    txt = open(test_path + BB_test_txt[i],encoding=\"utf8\").read()\n",
    "    documents += add_documents(txt, phrase_reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(documents, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5721"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eges9\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model[list(model.wv.vocab)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_embedding(phrase, model, processor):\n",
    "    original_phrase = phrase\n",
    "    phrase = phrase.replace(\"-\", \" \")\n",
    "    phrase = nltk.word_tokenize(phrase)\n",
    "    \n",
    "    phrase = [processor(word) for word in phrase if word.lower() not in stopwords.words(\"english\") and word.isalnum()]\n",
    "    for i in range(len(phrase)):\n",
    "        if phrase[i].isnumeric():\n",
    "            phrase[i] = \"##number##\"\n",
    "    \n",
    "    phrase_size = len(phrase)\n",
    "    vec_size = len(model[list(model.wv.vocab)[0]])\n",
    "    embedded_sum = np.zeros(vec_size)\n",
    "    if(phrase_size == 0):\n",
    "        print(original_phrase)\n",
    "        return -2\n",
    "    for word in phrase:\n",
    "        if word not in list(model.wv.vocab):\n",
    "            return -1\n",
    "        embedded_sum = embedded_sum + np.array(model[word])\n",
    "    embedded_sum = embedded_sum / phrase_size\n",
    "    \n",
    "    return embedded_sum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_reducer = lemmatize_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'related'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_reducer(\"related\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_habitat_map(processor, model):\n",
    "    habitat_list = ontology.split(\"\\n\\n[Term]\\n\")[1:]\n",
    "    habitat_map = {}\n",
    "    habitat_map_originals = {}\n",
    "    habitat_embeddings = {}\n",
    "    for h in habitat_list:\n",
    "        #new_habitat = Habitat()\n",
    "        h = h.split(\"\\n\")\n",
    "        oid = re.findall(r\"(?<=id: OBT:)[0-9]+\", h[0])[0]\n",
    "        name = (re.findall(r\"(?<=name: ).+\", h[1])[0])\n",
    "        original_name = name\n",
    "        if name != name.upper():\n",
    "            name = name.lower()\n",
    "            name = processor(name)\n",
    "    \n",
    "        habitat_map[name] = oid\n",
    "        habitat_map_originals[oid] = name\n",
    "        embedding = get_phrase_embedding(name, model, processor)\n",
    "        if type(embedding) == type(np.array([1])):\n",
    "            habitat_embeddings[name] = embedding\n",
    "        elif embedding == -2:\n",
    "            print(original_name)\n",
    "        \n",
    "        # word2vec buraya eklenmeli \n",
    "        for h_line in h:\n",
    "            if \"synonym\" in h_line:\n",
    "                synonym_name = (re.findall(r\"(?<=synonym: \\\").+(?=\\\")\", h_line)[0])\n",
    "                if synonym_name != synonym_name.upper():\n",
    "                    synonym_name = synonym_name.lower()\n",
    "                    synonym_name = processor(synonym_name)\n",
    "                if \"EXACT\" in h_line and synonym_name not in habitat_map:\n",
    "                    habitat_map[synonym_name] = oid\n",
    "                    habitat_embeddings[synonym_name] = embedding \n",
    "    return habitat_map, habitat_map_originals, habitat_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eges9\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\eges9\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can\n",
      "can\n"
     ]
    }
   ],
   "source": [
    "habitat_map, habitat_map_originals, habitat_embeddings = generate_habitat_map(phrase_reducer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_habitats(a1, a2, processor):\n",
    "    a2 = a2.split(\"\\n\")\n",
    "    a1 = a1.split(\"\\n\")\n",
    "    \n",
    "    for annotation in a2:\n",
    "        a1_line = 0\n",
    "        t = \"\"\n",
    "        oid = \"\"\n",
    "        #print(annotation)\n",
    "        if \"OntoBiotope\" in annotation:\n",
    "            t = re.findall(r\"(?<=Annotation:)T[0-9]+\", annotation)[0]\n",
    "            oid = re.findall(r\"(?<=Referent:OBT:)[0-9]+\", annotation)[0]\n",
    "            named_entity = \"\"\n",
    "            \n",
    "            while t not in a1[a1_line] and a1_line < len(a1)-1:\n",
    "                a1_line += 1\n",
    "            if \"Habitat\" in a1[a1_line]: \n",
    "                named_entity = (a1[a1_line].split(\"\\t\")[2])\n",
    "                if named_entity != named_entity.upper():\n",
    "                    named_entity = named_entity.lower()\n",
    "                    named_entity = processor(named_entity)\n",
    "                if named_entity not in habitat_map:\n",
    "                    habitat_map[named_entity] = oid\n",
    "                    #print(\"Newly added entity:\", named_entity, oid)\n",
    "                    #print(\"Original entity:\", habitat_map_originals[oid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(BB_train_txt)):    \n",
    "    a1 = open(train_path + BB_train_a1[i],encoding=\"utf8\").read()\n",
    "    a2 = open(train_path + BB_train_a2[i],encoding=\"utf8\").read()\n",
    "    add_habitats(a1, a2, phrase_reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_habitats_exact_matching(a1, txt, a1_name, processor, pred_file_name):\n",
    "    pred_file = open(pred_file_name, \"w\")\n",
    "    cands = 0\n",
    "    matches = 0\n",
    "    match_list = {}\n",
    "    a1 = a1.split(\"\\n\")\n",
    "    count = 1\n",
    "    for line in a1:\n",
    "        found = False\n",
    "        if \"Habitat\" in line:\n",
    "            cands += 1\n",
    "            name = (line.split(\"\\t\")[2])\n",
    "            named_entity = name\n",
    "            if name != name.upper():\n",
    "                name = name.lower()\n",
    "                named_entity = processor(name)\n",
    "            if named_entity in habitat_map:\n",
    "                matches += 1\n",
    "                match = name + \" - \" + habitat_map[named_entity] + \"---exact\"\n",
    "                match_list[line.split(\"\\t\")[0]] = [habitat_map[named_entity], named_entity, \"exact\"]\n",
    "                found = True\n",
    "            else:\n",
    "                #print(name)\n",
    "                pre_named_entity = get_headword(name)\n",
    "                #print(pre_named_entity)\n",
    "                named_entity = processor(pre_named_entity)\n",
    "                if named_entity in habitat_map:\n",
    "                    matches += 1\n",
    "                    match = name + \" - \" + habitat_map[named_entity] + \"---headwordexact\"\n",
    "                    print(match)\n",
    "                    match_list[line.split(\"\\t\")[0]] = [habitat_map[named_entity], name, \"headwordexact\", named_entity]\n",
    "                    found = True\n",
    "        if found:\n",
    "            pred_file.write(\"N\" + str(count) +\"\\tOntoBiotope Annotation:\" + line.split(\"\\t\")[0] + \" Referent:OBT:\" + match_list[line.split(\"\\t\")[0]][0] + \"\\n\")\n",
    "            count += 1\n",
    "    #print(a1_name)\n",
    "    #print(\"Out of\", cands, \"candidates,\", matches, \"matches found\")\n",
    "    #print()\n",
    "    pred_file.close()\n",
    "    return match_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1348"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_habitats(a1, txt, a1_name, processor, pred_file_name, model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_test = {}\n",
    "for i in range(len(BB_dev_txt)):\n",
    "    a1 = open(dev_path + BB_dev_a1[i],encoding=\"utf8\").read()\n",
    "    txt = open(dev_path + BB_dev_txt[i],encoding=\"utf8\").read()\n",
    "    pred_file_name = \"./dev_preds/\" + BB_dev_a2[i]\n",
    "    annotations_test[BB_dev_a1[i]] = predict_habitats(a1, txt, BB_dev_a1[i], phrase_reducer, pred_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_habitats(a1, a2, processor):\n",
    "    \n",
    "    a2 = a2.split(\"\\n\")\n",
    "    a1 = a1.split(\"\\n\")\n",
    "    match_list = {}\n",
    "    for annotation in a2:\n",
    "        a1_line = 0\n",
    "        t = \"\"\n",
    "        oid = \"\"\n",
    "        #print(annotation)\n",
    "        if \"OntoBiotope\" in annotation:\n",
    "            t = re.findall(r\"(?<=Annotation:)T[0-9]+\", annotation)[0]\n",
    "            oid = re.findall(r\"(?<=Referent:OBT:)[0-9]+\", annotation)[0]\n",
    "            named_entity = \"\"\n",
    "            while t not in a1[a1_line] and a1_line < len(a1)-1:\n",
    "                a1_line += 1\n",
    "            if \"Habitat\" in a1[a1_line]:\n",
    "                named_entity = (a1[a1_line].split(\"\\t\")[2])\n",
    "                if named_entity != named_entity.upper():\n",
    "                    named_entity = named_entity.lower()\n",
    "                    named_entity = processor(named_entity)\n",
    "                match_list[t] = [oid, named_entity]\n",
    "    return match_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_a2 = {}\n",
    "for i in range(len(BB_dev_txt)):\n",
    "    a1 = open(dev_path + BB_dev_a1[i],encoding=\"utf8\").read()\n",
    "    a2 = open(dev_path + BB_dev_a2[i],encoding=\"utf8\").read()\n",
    "    dev_a2[BB_dev_a1[i]] = test_habitats(a1,a2, phrase_reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_pred_count = 0\n",
    "false_pred_count = 0\n",
    "total_count = 0\n",
    "for key in dev_a2:\n",
    "    \"\"\"print()\n",
    "    print(key)\n",
    "    print(dev_a2[key])\n",
    "    print(\"**************\")\"\"\"\n",
    "    total_count += len(dev_a2[key])\n",
    "    if key in annotations_test:\n",
    "        for pred in annotations_test[key]:            \n",
    "            if annotations_test[key][pred][0] == dev_a2[key][pred][0]:\n",
    "                true_pred_count += 1\n",
    "            else:\n",
    "                print(key)\n",
    "                print(\"Prediction:\", annotations_test[key][pred],\" Real:\", dev_a2[key][pred])\n",
    "                print(\"Prediction:\", habitat_map_originals[annotations_test[key][pred][0]], \" Real:\", habitat_map_originals[dev_a2[key][pred][0]])\n",
    "                false_pred_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recall = true_pred_count/total_count\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = true_pred_count/(true_pred_count + false_pred_count)\n",
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denemeler"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
